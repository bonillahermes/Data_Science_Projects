{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMa/EIwqN/dyXacEYLZXj0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonillahermes/Data_Science_Projects/blob/main/ModelosClasPred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hermes Yate Bonilla\n",
        "**Data Scientist**\n",
        "---\n",
        "\n",
        "**Contact:**\n",
        "- **Email:** [bonillahermes@gmail.com](mailto:bonillahermes@gmail.com)\n",
        "- **LinkedIn:** [linkedin.com/in/bonillahermes](https://www.linkedin.com/in/bonillahermes/)\n",
        "- **GitHub:** [github.com/bonillahermes](https://github.com/bonillahermes)\n",
        "- **Webpage:** [bonillahermes.com](https://bonillahermes.com/)\n",
        "---"
      ],
      "metadata": {
        "id": "ty99PRRAHyX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos de Clasificación y Predicción"
      ],
      "metadata": {
        "id": "rRKmma4uH0vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminares"
      ],
      "metadata": {
        "id": "JDNr5OiGTuvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerías\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "qKDjDlRRq-L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerías generales\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import stats\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "# Importación de bibliotecas de machine learning de scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, precision_score, recall_score,\n",
        "    roc_curve, roc_auc_score, classification_report, precision_recall_curve, average_precision_score,\n",
        "    mean_squared_error, r2_score, silhouette_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "\n",
        "# Importación de XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Importación de bibliotecas para modelos y redes neuronales\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Importación de Keras Tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Importación de bibliotecas para visualización interactiva\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "4zgvuUni3tUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 1"
      ],
      "metadata": {
        "id": "OApVkMBTNxLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Base.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "sheet_name = ('Mod1') # Nombres de las hoja\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)"
      ],
      "metadata": {
        "id": "xrIZH1IH3x_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "rDXWHzpJ7bwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rNAsLaW742_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "i3LBzZkW458u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depuración"
      ],
      "metadata": {
        "id": "Pc3k-PDjfbjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el porcentaje de valores faltantes para cada columna\n",
        "missing_percentage = df.isnull().mean() * 100\n",
        "\n",
        "# Filtrar las columnas que tienen menos del 30% de valores faltantes\n",
        "columns_to_keep = missing_percentage[missing_percentage < 30].index\n",
        "\n",
        "# Crear un nuevo DataFrame solo con las columnas seleccionadas\n",
        "df = df[columns_to_keep]"
      ],
      "metadata": {
        "id": "5koearYo6aHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Weh4noWL7JUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "LGQbJo7I6lJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputación"
      ],
      "metadata": {
        "id": "tJUgq5-5q-1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' fuera de las columnas que serán imputadas\n",
        "nuid_column = df['NORDEMP']\n",
        "ciiu_column = df['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la imputación\n",
        "df_for_imputation = df.drop(columns=['NORDEMP', 'CIIU 4'])"
      ],
      "metadata": {
        "id": "NwN_1I1E3oQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar valores iniciales con KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_for_imputation = pd.DataFrame(knn_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)\n",
        "\n",
        "# Imputar datos usando IterativeImputer (similar a MICE)\n",
        "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "df_for_imputation = pd.DataFrame(iterative_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)"
      ],
      "metadata": {
        "id": "OESA-24ZlewO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df_for_imputation.isnull().sum()"
      ],
      "metadata": {
        "id": "BTotQlC7Uh7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volver a añadir la columna 'NORDEMP' al DataFrame\n",
        "df_for_imputation['NORDEMP'] = nuid_column\n",
        "df_for_imputation['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Guardar cambios\n",
        "df = df_for_imputation"
      ],
      "metadata": {
        "id": "f831I_GMiesM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame resultante en un archivo Excel llamado Mod1.xlsx\n",
        "output_file = 'Mod1.xlsx'\n",
        "df.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "x8823QPrUrnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducción de Dimensionalidad"
      ],
      "metadata": {
        "id": "GgvJAxT6qKOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminar"
      ],
      "metadata": {
        "id": "JTIp9ZZWjpW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular la distancia de Mahalanobis\n",
        "def mahalanobis_distances(components):\n",
        "    mean = np.mean(components, axis=0)\n",
        "    cov = np.cov(components.T)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    distances = [mahalanobis(comp, mean, inv_cov) for comp in components]\n",
        "    return np.array(distances)"
      ],
      "metadata": {
        "id": "m2LIePTIjmYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod1_reduced = Mod1.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "Mod1_scaled = scaler.fit_transform(Mod1_reduced)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components= 10)\n",
        "pca.fit(Mod1_scaled)\n",
        "\n",
        "# Obtener la varianza explicada por cada componente\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Crear un DataFrame para la varianza explicada\n",
        "explained_variance_df = pd.DataFrame({\n",
        "    'Component': range(1, len(explained_variance) + 1),\n",
        "    'Explained Variance': explained_variance\n",
        "})\n",
        "\n",
        "# Mostrar la varianza explicada por cada componente\n",
        "print(explained_variance_df)"
      ],
      "metadata": {
        "id": "T41vICDThxih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar Base"
      ],
      "metadata": {
        "id": "uDX7Dnvpkdnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod1.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod1 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "9DVcatLOVjZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mod1.head()"
      ],
      "metadata": {
        "id": "W7qMyD3okySm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### t-SNE"
      ],
      "metadata": {
        "id": "REvJznT1klOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' y 'CIIU 4' fuera\n",
        "nuid_column = Mod1['NORDEMP']\n",
        "ciiu_column = Mod1['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la reducción\n",
        "Mod1_red = Mod1.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Preprocesamiento: eliminar filas con valores faltantes\n",
        "Mod1_red.dropna(inplace=True)\n",
        "\n",
        "# Aplicar t-SNE para reducir la dimensionalidad a 2 componentes\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(Mod1_red)\n",
        "\n",
        "# Crear un DataFrame con los resultados de t-SNE\n",
        "df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
        "df_tsne['NORDEMP'] = nuid_column\n",
        "df_tsne['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', data=df_tsne, palette='viridis')\n",
        "plt.title('t-SNE results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pjtL2D3Fklax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBSCAN"
      ],
      "metadata": {
        "id": "p8VjjxJSwc62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar DBSCAN para detectar outliers\n",
        "dbscan = DBSCAN(eps=3, min_samples=10)  # Ajusta los parámetros según tus datos\n",
        "df_tsne['outlier'] = dbscan.fit_predict(tsne_results)\n",
        "df_tsne['index'] = Mod1_red.index\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='outlier', palette='viridis', data=df_tsne, legend='full')\n",
        "plt.title('t-SNE results with outliers marked')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uROI26lyufU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eliminación Outliers"
      ],
      "metadata": {
        "id": "4p1KZu5skwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los datos atípicos\n",
        "outliers = df_tsne[df_tsne['outlier'] == -1]\n",
        "print(\"Datos atípicos detectados:\")\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "op5-DeyquoLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar los índices de los datos que no son outliers\n",
        "non_outliers_indices = df_tsne[df_tsne['outlier'] != -1]['index']\n",
        "\n",
        "# Crear un nuevo DataFrame sin los outliers\n",
        "Mod1_cleaned = Mod1.loc[non_outliers_indices]\n",
        "\n",
        "# Guardar el DataFrame limpio en un nuevo archivo Excel\n",
        "Mod1_cleaned.to_excel('Mod1_cleaned.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "yyg4xtOYvS0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mod1 = Mod1_cleaned"
      ],
      "metadata": {
        "id": "eV3gU15QvbpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mod1.head()"
      ],
      "metadata": {
        "id": "A01jucxZwDOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mod1.isnull().sum()"
      ],
      "metadata": {
        "id": "jptEyDVswHxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Clasificación"
      ],
      "metadata": {
        "id": "oWdQszmpUV61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descripción y recodificación"
      ],
      "metadata": {
        "id": "MZEqoCUPEIKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod1_cleaned.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod1 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "oizT8rXJLffF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una tabla de frecuencias para la variable I2R5C1\n",
        "tabla_frecuencias = Mod1['I2R5C1'].value_counts().sort_index()\n",
        "\n",
        "# Crear un DataFrame con la tabla de frecuencias para mayor claridad\n",
        "tabla_frecuencias_df = pd.DataFrame(tabla_frecuencias).reset_index()\n",
        "tabla_frecuencias_df.columns = ['Productividad', 'Frecuencia']\n",
        "\n",
        "# Mostrar la tabla de frecuencias\n",
        "print(tabla_frecuencias_df)"
      ],
      "metadata": {
        "id": "tdv8aELnzS-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recodificar la variable I2R5C1: 1 se mantiene igual, 2 y 3 se recodifican a 0\n",
        "Mod1['I2R5C1'] = Mod1['I2R5C1'].replace({2: 0, 3: 0})"
      ],
      "metadata": {
        "id": "-1B3YHr6DoP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una tabla de frecuencias para la variable I2R5C1\n",
        "tabla_frecuencias = Mod1['I2R5C1'].value_counts().sort_index()\n",
        "\n",
        "# Crear un DataFrame con la tabla de frecuencias para mayor claridad\n",
        "tabla_frecuencias_df = pd.DataFrame(tabla_frecuencias).reset_index()\n",
        "tabla_frecuencias_df.columns = ['Productividad', 'Frecuencia']\n",
        "\n",
        "# Mostrar la tabla de frecuencias\n",
        "print(tabla_frecuencias_df)"
      ],
      "metadata": {
        "id": "bScVKlLGDspN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelos"
      ],
      "metadata": {
        "id": "PfymuqEVFw8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar las características (todas menos 'I2R5C1' y 'I2R5C1_recoded') y la variable objetivo ('I2R5C1_recoded')\n",
        "X = Mod1.drop(columns=['I2R5C1'])\n",
        "y = Mod1['I2R5C1']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BCilL8eKFwCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Redes Neuronales"
      ],
      "metadata": {
        "id": "dS-xYZoGIExA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función de creación de modelo\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=564, step=32),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\"])\n",
        "        ))\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(layers.Dropout(rate=0.25))\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.binary_crossentropy,\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "GxMltsjiGnnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Búsqueda de hiperparámetros\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=10,\n",
        "    directory=\"/content/Tuning_Productividad\",\n",
        "    project_name=\"modelo_productividad\"\n",
        ")"
      ],
      "metadata": {
        "id": "z-3pfPZ9GANH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "nTehjFDoGQga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con los mejores hiperparámetros\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "MA36ELcNGTeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo\n",
        "eval_result = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "# Precisión en los datos de prueba\n",
        "accuracy = eval_result[1]\n",
        "print('Precisión en los datos de prueba:', accuracy)"
      ],
      "metadata": {
        "id": "uJfMMImTGXCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo localmente\n",
        "model.save('/content/modelo_productividad.h5')"
      ],
      "metadata": {
        "id": "0n9qBGwkGYic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regresión Logística"
      ],
      "metadata": {
        "id": "phBNrqrMIKpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Regresión Logística\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_logreg = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['lbfgs', 'liblinear', 'saga'],\n",
        "}\n",
        "\n",
        "# Ajustar el GridSearchCV para la regresión logística\n",
        "grid_search_logreg = GridSearchCV(\n",
        "    LogisticRegression(max_iter=10000),\n",
        "    param_grid_logreg,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "ZuYzwrEFKX4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_logreg.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "Xgz1PHvTIOOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los mejores hiperparámetros\n",
        "best_logreg = grid_search_logreg.best_estimator_"
      ],
      "metadata": {
        "id": "Ff9B7xZuJGbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_logreg.fit(X_train_scaled, y_train)\n",
        "y_pred_logreg = best_logreg.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Mejores hiperparámetros para Regresión Logística:\")\n",
        "print(grid_search_logreg.best_params_)\n",
        "print(\"Reporte de clasificación para Regresión Logística:\")\n",
        "print(classification_report(y_test, y_pred_logreg))"
      ],
      "metadata": {
        "id": "xVk5xz3ZJIaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Regresión Logística con los mejores hiperparámetros\n",
        "best_logreg = LogisticRegression(C=0.1, penalty='l1', solver='saga', max_iter=5000)\n",
        "best_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_logreg = best_logreg.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para Regresión Logística con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_logreg))"
      ],
      "metadata": {
        "id": "ax_Zl1DcXQ-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "dz8KQ29hK7u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_rf = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Mejores hiperparámetros para Random Forest:\")\n",
        "print(grid_search_rf.best_params_)\n",
        "print(\"Reporte de clasificación para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "G50Gk1D3K-YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Random Forest con los mejores hiperparámetros\n",
        "best_rf = RandomForestClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=30,\n",
        "    max_features='sqrt',\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=5,\n",
        "    n_estimators=100\n",
        ")\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para Random Forest con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "OMWXj4UXvScr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBOOST"
      ],
      "metadata": {
        "id": "uoOTfwKXMSxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de XGBoost\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')"
      ],
      "metadata": {
        "id": "EoyIlcVTgNIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el grid de hiperparámetros\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Mejores hiperparámetros para XGBoost:\")\n",
        "print(grid_search_xgb.best_params_)\n",
        "print(\"Reporte de clasificación para XGBoost:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "a-oYI65XMVD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de XGBoost con los mejores hiperparámetros\n",
        "best_xgb = xgb.XGBClassifier(\n",
        "    colsample_bytree=0.7,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    n_estimators=100,\n",
        "    subsample=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para XGBoost con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "skqXyAcVvQ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2"
      ],
      "metadata": {
        "id": "nBo4MW4-q55o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "G0IfL84Rj-Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Base.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "sheet_name = ('Mod2') # Nombres de las hoja\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)"
      ],
      "metadata": {
        "id": "Ki4_YlW3U_xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "XX9b8WV7VLHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BUhlD-xjVTOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "YOcoGJBGVXX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depuración"
      ],
      "metadata": {
        "id": "dSiFJVgMkDrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el porcentaje de valores faltantes para cada columna\n",
        "missing_percentage = df.isnull().mean() * 100\n",
        "\n",
        "# Filtrar las columnas que tienen menos del 30% de valores faltantes\n",
        "columns_to_keep = missing_percentage[missing_percentage < 30].index\n",
        "\n",
        "# Crear un nuevo DataFrame solo con las columnas seleccionadas\n",
        "df = df[columns_to_keep]"
      ],
      "metadata": {
        "id": "Jec0EVMRVbLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "pFmQPmfpVi9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "rtfdVqw5VqSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputación"
      ],
      "metadata": {
        "id": "U6lFVv-fkKnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' fuera de las columnas que serán imputadas\n",
        "nuid_column = df['NORDEMP']\n",
        "ciiu_column = df['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la imputación\n",
        "df_for_imputation = df.drop(columns=['NORDEMP', 'CIIU 4'])"
      ],
      "metadata": {
        "id": "BfY7G_pRVrok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar valores iniciales con KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_for_imputation = pd.DataFrame(knn_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)\n",
        "\n",
        "# Imputar datos usando IterativeImputer (similar a MICE)\n",
        "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "df_for_imputation = pd.DataFrame(iterative_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)"
      ],
      "metadata": {
        "id": "dw6jli0zWZjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df_for_imputation.isnull().sum()"
      ],
      "metadata": {
        "id": "eIG6c0KvWdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volver a añadir la columna 'NORDEMP' al DataFrame\n",
        "df_for_imputation['NORDEMP'] = nuid_column\n",
        "df_for_imputation['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Guardar cambios\n",
        "df = df_for_imputation"
      ],
      "metadata": {
        "id": "4N9sXrOTWnZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame resultante en un archivo Excel llamado Mod2.xlsx\n",
        "output_file = 'Mod2.xlsx'\n",
        "df.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "gCW4sSc9XeWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducción de Dimensionalidad"
      ],
      "metadata": {
        "id": "kv_1rwwnkT4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod2.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod2 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "UVcH8A_hZj9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod2_reduced = Mod2.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "Mod2_scaled = scaler.fit_transform(Mod2_reduced)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components= 10)\n",
        "pca.fit(Mod2_scaled)\n",
        "\n",
        "# Obtener la varianza explicada por cada componente\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Crear un DataFrame para la varianza explicada\n",
        "explained_variance_df = pd.DataFrame({\n",
        "    'Component': range(1, len(explained_variance) + 1),\n",
        "    'Explained Variance': explained_variance\n",
        "})\n",
        "\n",
        "# Mostrar la varianza explicada por cada componente\n",
        "print(explained_variance_df)"
      ],
      "metadata": {
        "id": "e5anhwk5hDbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar la varianza explicada acumulada\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--')\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('Varianza Explicada Acumulada')\n",
        "plt.title('Varianza Explicada Acumulada por el PCA')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2R8zVgDhNZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### t-SNE"
      ],
      "metadata": {
        "id": "L7mMlX7Yk7uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' y 'CIIU 4' fuera\n",
        "nuid_column = Mod2['NORDEMP']\n",
        "ciiu_column = Mod2['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la reducción\n",
        "Mod2_red = Mod2.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Preprocesamiento: eliminar filas con valores faltantes\n",
        "Mod2_red.dropna(inplace=True)\n",
        "\n",
        "# Aplicar t-SNE para reducir la dimensionalidad a 2 componentes\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(Mod2_red)\n",
        "\n",
        "# Crear un DataFrame con los resultados de t-SNE\n",
        "df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
        "df_tsne['NORDEMP'] = nuid_column\n",
        "df_tsne['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', data=df_tsne, palette='viridis')\n",
        "plt.title('t-SNE results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ajEbbSoiIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBSCAN"
      ],
      "metadata": {
        "id": "R-xxmqhylBQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar DBSCAN para detectar outliers\n",
        "dbscan = DBSCAN(eps=3, min_samples=10)  # Ajusta los parámetros según tus datos\n",
        "df_tsne['outlier'] = dbscan.fit_predict(tsne_results)\n",
        "df_tsne['index'] = Mod2_red.index\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='outlier', palette='viridis', data=df_tsne, legend='full')\n",
        "plt.title('t-SNE results with outliers marked')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uEhtSkopiti8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eliminación de Outliers"
      ],
      "metadata": {
        "id": "zTfzzjrolIm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los datos atípicos\n",
        "outliers = df_tsne[df_tsne['outlier'] == -1]\n",
        "print(\"Datos atípicos detectados:\")\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "nUGL0Ns5jEzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar los índices de los datos que no son outliers\n",
        "non_outliers_indices = df_tsne[df_tsne['outlier'] != -1]['index']\n",
        "\n",
        "# Crear un nuevo DataFrame sin los outliers\n",
        "Mod2_cleaned = Mod2.loc[non_outliers_indices]\n",
        "\n",
        "# Guardar el DataFrame limpio en un nuevo archivo Excel\n",
        "Mod2_cleaned.to_excel('Mod2_cleaned.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "N-0DbTo8jGyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mod2 = Mod2_cleaned"
      ],
      "metadata": {
        "id": "k9FX7AdemMIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Clasificación"
      ],
      "metadata": {
        "id": "YzKsC_uij6UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod2_cleaned.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod2 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "fA2iKYPPvl5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una tabla de frecuencias para la variable I10R7C1\n",
        "tabla_frecuencias = Mod2['I10R7C1'].value_counts().sort_index()\n",
        "\n",
        "# Crear un DataFrame con la tabla de frecuencias para mayor claridad\n",
        "tabla_frecuencias_df = pd.DataFrame(tabla_frecuencias).reset_index()\n",
        "tabla_frecuencias_df.columns = ['Incertidumbre', 'Frecuencia']\n",
        "\n",
        "# Mostrar la tabla de frecuencias\n",
        "print(tabla_frecuencias_df)"
      ],
      "metadata": {
        "id": "tQ286ZfWlzEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod2_reduced = Mod2.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Seleccionar las características (todas menos 'I10R7C1') y la variable objetivo ('I10R7C1')\n",
        "X = Mod2_reduced.drop(columns=['I10R7C1'])\n",
        "y = Mod2_reduced['I10R7C1']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BgSG9QtlonTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Regresión Logística Multinomial\n",
        "logreg = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_logreg = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'l1_ratio': [0, 0.5, 1]  # Solo aplicable si la penalización es 'elasticnet'\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_logreg = grid_search_logreg.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_logreg.fit(X_train_scaled, y_train)\n",
        "y_pred_logreg = best_logreg.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Mejores hiperparámetros para Regresión Logística Multinomial:\")\n",
        "print(grid_search_logreg.best_params_)\n",
        "print(\"Reporte de clasificación para Regresión Logística Multinomial:\")\n",
        "print(classification_report(y_test, y_pred_logreg))"
      ],
      "metadata": {
        "id": "l1NeqkCZo0KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Regresión Logística Multinomial con los mejores hiperparámetros\n",
        "best_logreg = LogisticRegression(\n",
        "    C=0.01,\n",
        "    penalty='l1',\n",
        "    solver='saga',\n",
        "    multi_class='multinomial',\n",
        "    max_iter=5000\n",
        ")\n",
        "best_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_logreg = best_logreg.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para Regresión Logística Multinomial con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_logreg))"
      ],
      "metadata": {
        "id": "3VwF-CCOOLmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Mejores hiperparámetros para Random Forest:\")\n",
        "print(grid_search_rf.best_params_)\n",
        "print(\"Reporte de clasificación para Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "HFzLEwPeo2zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Random Forest con los mejores hiperparámetros\n",
        "best_rf = RandomForestClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    max_features='auto',\n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=5,\n",
        "    n_estimators=300\n",
        ")\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para Random Forest con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "5Oru9EezbpH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar las características (todas menos 'I10R7C1') y la variable objetivo ('I10R7C1')\n",
        "X = Mod2_reduced.drop(columns=['I10R7C1'])\n",
        "y = Mod2_reduced['I10R7C1']\n",
        "\n",
        "# Recodificar las etiquetas de '1, 2, 3' a '0, 1, 2'\n",
        "y = y - 1\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Definir la función de creación de modelo\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        ))\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(layers.Dense(3, activation=\"softmax\"))  # 3 clases para clasificación multiclase\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Búsqueda de hiperparámetros\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=20,\n",
        "    directory=\"/content/tuning_neural_network\",\n",
        "    project_name=\"NN_tuning\"\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Evaluar el modelo\n",
        "eval_result = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "# Precisión en los datos de prueba\n",
        "accuracy = eval_result[1]\n",
        "print('Precisión en los datos de prueba:', accuracy)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "y_pred_nn_classes = y_pred_nn.argmax(axis=-1)\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "print(\"Reporte de clasificación para Redes Neuronales con mejores hiperparámetros:\")\n",
        "print(classification_report(y_test, y_pred_nn_classes))\n"
      ],
      "metadata": {
        "id": "PKT-iRmbf6G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 3"
      ],
      "metadata": {
        "id": "r0gFcSbH7Ke3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "_X3LATsY8RTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Base.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "sheet_name = ('Mod3') # Nombres de las hoja\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)"
      ],
      "metadata": {
        "id": "wOrZwFG67p9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "9a-JRQZT8GDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vekkOt5i8HEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8YXMuNsQ8K9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depuración"
      ],
      "metadata": {
        "id": "_j90s2LJ8Tzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el porcentaje de valores faltantes para cada columna\n",
        "missing_percentage = df.isnull().mean() * 100\n",
        "\n",
        "# Filtrar las columnas que tienen menos del 30% de valores faltantes\n",
        "columns_to_keep = missing_percentage[missing_percentage < 30].index\n",
        "\n",
        "# Crear un nuevo DataFrame solo con las columnas seleccionadas\n",
        "df = df[columns_to_keep]"
      ],
      "metadata": {
        "id": "Wb2vOtaT87kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "IwzMdkl58_ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Zime-0ry9C4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputación"
      ],
      "metadata": {
        "id": "phxr5Doi9IFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' fuera de las columnas que serán imputadas\n",
        "nuid_column = df['NORDEMP']\n",
        "ciiu_column = df['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la imputación\n",
        "df_for_imputation = df.drop(columns=['NORDEMP', 'CIIU 4'])"
      ],
      "metadata": {
        "id": "6wEWuF4Y9HqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar valores iniciales con KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_for_imputation = pd.DataFrame(knn_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)\n",
        "\n",
        "# Imputar datos usando IterativeImputer (similar a MICE)\n",
        "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "df_for_imputation = pd.DataFrame(iterative_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)"
      ],
      "metadata": {
        "id": "w6_NC9Lh9mnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df_for_imputation.isnull().sum()"
      ],
      "metadata": {
        "id": "yb6-Ph0_9nf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volver a añadir la columna 'NORDEMP' al DataFrame\n",
        "df_for_imputation['NORDEMP'] = nuid_column\n",
        "df_for_imputation['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Guardar cambios\n",
        "df = df_for_imputation"
      ],
      "metadata": {
        "id": "6yQEpFNR-VVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame resultante en un archivo Excel llamado Mod3.xlsx\n",
        "output_file = 'Mod3.xlsx'\n",
        "df.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "5FSioMMt-YyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducción de dimensionalidad"
      ],
      "metadata": {
        "id": "41T_vjt_-0H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod3.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod3 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "iL2dE6Uh-xdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod3_reduced = Mod3.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "Mod3_scaled = scaler.fit_transform(Mod3_reduced)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components= 10)\n",
        "pca.fit(Mod3_scaled)\n",
        "\n",
        "# Obtener la varianza explicada por cada componente\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Crear un DataFrame para la varianza explicada\n",
        "explained_variance_df = pd.DataFrame({\n",
        "    'Component': range(1, len(explained_variance) + 1),\n",
        "    'Explained Variance': explained_variance\n",
        "})\n",
        "\n",
        "# Mostrar la varianza explicada por cada componente\n",
        "print(explained_variance_df)"
      ],
      "metadata": {
        "id": "FEtBisys_Vic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar la varianza explicada acumulada\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--')\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('Varianza Explicada Acumulada')\n",
        "plt.title('Varianza Explicada Acumulada por el PCA')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uc0e4F9H_4I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE"
      ],
      "metadata": {
        "id": "QqSIMr8Jd0B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' y 'CIIU 4' fuera\n",
        "nuid_column = Mod3['NORDEMP']\n",
        "ciiu_column = Mod3['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la reducción\n",
        "Mod3_red = Mod3.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Preprocesamiento: eliminar filas con valores faltantes\n",
        "Mod3_red.dropna(inplace=True)\n",
        "\n",
        "# Aplicar t-SNE para reducir la dimensionalidad a 2 componentes\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(Mod3_red)\n",
        "\n",
        "# Crear un DataFrame con los resultados de t-SNE\n",
        "df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
        "df_tsne['NORDEMP'] = nuid_column\n",
        "df_tsne['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', data=df_tsne, palette='viridis')\n",
        "plt.title('t-SNE results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UMbziPdGd24J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "3x2RM5v2d3b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar DBSCAN para detectar outliers\n",
        "dbscan = DBSCAN(eps=3, min_samples=10)  # Ajusta los parámetros según tus datos\n",
        "df_tsne['outlier'] = dbscan.fit_predict(tsne_results)\n",
        "df_tsne['index'] = Mod3_red.index\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='outlier', palette='viridis', data=df_tsne, legend='full')\n",
        "plt.title('t-SNE results with outliers marked')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aZv4H3dbd6zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminación de outliers"
      ],
      "metadata": {
        "id": "XYG1naEMd7Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los datos atípicos\n",
        "outliers = df_tsne[df_tsne['outlier'] == -1]\n",
        "print(\"Datos atípicos detectados:\")\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "36bXtDGqeABY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar los índices de los datos que no son outliers\n",
        "non_outliers_indices = df_tsne[df_tsne['outlier'] != -1]['index']\n",
        "\n",
        "# Crear un nuevo DataFrame sin los outliers\n",
        "Mod3_cleaned = Mod3.loc[non_outliers_indices]\n",
        "\n",
        "# Guardar el DataFrame limpio en un nuevo archivo Excel\n",
        "Mod3_cleaned.to_excel('Mod3_cleaned.xlsx', index=False)"
      ],
      "metadata": {
        "id": "-QErPPhFeSE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Predicción"
      ],
      "metadata": {
        "id": "mT0fKHJ3eXya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod3_cleaned.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod3 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "pMYWPm10jhJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod3 = Mod3.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Seleccionar las características (todas menos 'II1R3C2') y la variable objetivo ('II1R3C2')\n",
        "X = Mod3.drop(columns=['II1R3C2'])\n",
        "y = Mod3['II1R3C2']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "cUWTebufjW-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Regresión Lineal\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_lin_reg = lin_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_lin_reg = mean_squared_error(y_test, y_pred_lin_reg)\n",
        "r2_lin_reg = r2_score(y_test, y_pred_lin_reg)\n",
        "\n",
        "print(\"Regresión Lineal\")\n",
        "print(\"Mean Squared Error:\", mse_lin_reg)\n",
        "print(\"R2 Score:\", r2_lin_reg)"
      ],
      "metadata": {
        "id": "-Fkt-kIXka0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Random Forest\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"Mejores hiperparámetros:\", grid_search_rf.best_params_)\n",
        "print(\"Mean Squared Error:\", mse_rf)\n",
        "print(\"R2 Score:\", r2_rf)"
      ],
      "metadata": {
        "id": "B_um57bnlDeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de Random Forest con los mejores hiperparámetros\n",
        "best_rf = RandomForestRegressor(\n",
        "    max_depth=20,\n",
        "    max_features='auto',\n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=2,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"Mean Squared Error:\", mse_rf)\n",
        "print(\"R2 Score:\", r2_rf)"
      ],
      "metadata": {
        "id": "J4YDI-xdPRpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de XGBoost\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"Mejores hiperparámetros:\", grid_search_xgb.best_params_)\n",
        "print(\"Mean Squared Error:\", mse_xgb)\n",
        "print(\"R2 Score:\", r2_xgb)\n"
      ],
      "metadata": {
        "id": "Jt8rMDbTFsL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de XGBoost con los mejores hiperparámetros\n",
        "best_xgb = xgb.XGBRegressor(\n",
        "    colsample_bytree=0.9,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    n_estimators=300,\n",
        "    subsample=0.7,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"Mean Squared Error:\", mse_xgb)\n",
        "print(\"R2 Score:\", r2_xgb)"
      ],
      "metadata": {
        "id": "9B9QbAW1na6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función de creación de modelo\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        ))\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(layers.Dense(1))  # Salida para regresión\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        metrics=['mean_squared_error']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Búsqueda de hiperparámetros\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_mean_squared_error\",\n",
        "    max_trials=20,\n",
        "    directory=\"/content/tuning_neural_network\",\n",
        "    project_name=\"NN_tuning\"\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Evaluar el modelo\n",
        "eval_result = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "# Precisión en los datos de prueba\n",
        "mse = eval_result[0]\n",
        "r2 = r2_score(y_test, model.predict(X_test_scaled))\n",
        "\n",
        "print('Mean Squared Error en los datos de prueba:', mse)\n",
        "print('R2 Score en los datos de prueba:', r2)"
      ],
      "metadata": {
        "id": "a6_bC3o8Gz2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 4"
      ],
      "metadata": {
        "id": "fCjOLzv3AU8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "KP_5uxtKAd-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Base.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "sheet_name = ('Mod4') # Nombres de las hoja\n",
        "df = pd.read_excel(file_path, sheet_name=sheet_name)"
      ],
      "metadata": {
        "id": "YzjACTSkAa6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "9JVqp3xAAnlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0_9r8J11ArR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "aZJEHCVlAueD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depuración"
      ],
      "metadata": {
        "id": "JvHIi1jvBU29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el porcentaje de valores faltantes para cada columna\n",
        "missing_percentage = df.isnull().mean() * 100\n",
        "\n",
        "# Filtrar las columnas que tienen menos del 30% de valores faltantes\n",
        "columns_to_keep = missing_percentage[missing_percentage < 30].index\n",
        "\n",
        "# Crear un nuevo DataFrame solo con las columnas seleccionadas\n",
        "df = df[columns_to_keep]"
      ],
      "metadata": {
        "id": "NEGFf_cSBXIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la dimensión del Dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "BJWjBgh8CvTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "yob5CwM_CvDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputación"
      ],
      "metadata": {
        "id": "fd24kB8dChZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' fuera de las columnas que serán imputadas\n",
        "nuid_column = df['NORDEMP']\n",
        "ciiu_column = df['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la imputación\n",
        "df_for_imputation = df.drop(columns=['NORDEMP', 'CIIU 4'])"
      ],
      "metadata": {
        "id": "QETzrEEHCqwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar valores iniciales con KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df_for_imputation = pd.DataFrame(knn_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)\n",
        "\n",
        "# Imputar datos usando IterativeImputer (similar a MICE)\n",
        "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "df_for_imputation = pd.DataFrame(iterative_imputer.fit_transform(df_for_imputation), columns=df_for_imputation.columns)"
      ],
      "metadata": {
        "id": "kbu7VTa1BYN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el número de valores faltantes por columna\n",
        "df_for_imputation.isnull().sum()"
      ],
      "metadata": {
        "id": "rxeX4tarCFI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Volver a añadir la columna 'NORDEMP' al DataFrame\n",
        "df_for_imputation['NORDEMP'] = nuid_column\n",
        "df_for_imputation['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Guardar cambios\n",
        "df = df_for_imputation"
      ],
      "metadata": {
        "id": "XQxo_dwPCI98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame resultante en un archivo Excel llamado Mod4.xlsx\n",
        "output_file = 'Mod4.xlsx'\n",
        "df.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "2_lBUv2HCNx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducción de dimensionalidad"
      ],
      "metadata": {
        "id": "AvTgz44uDAOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod4.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod4 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "1-7OBZ1SDQRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod4_reduced = Mod4.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "Mod4_scaled = scaler.fit_transform(Mod4_reduced)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA(n_components= 10)\n",
        "pca.fit(Mod4_scaled)\n",
        "\n",
        "# Obtener la varianza explicada por cada componente\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Crear un DataFrame para la varianza explicada\n",
        "explained_variance_df = pd.DataFrame({\n",
        "    'Component': range(1, len(explained_variance) + 1),\n",
        "    'Explained Variance': explained_variance\n",
        "})\n",
        "\n",
        "# Mostrar la varianza explicada por cada componente\n",
        "print(explained_variance_df)"
      ],
      "metadata": {
        "id": "Hdp8kCt6DV4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE"
      ],
      "metadata": {
        "id": "fd1yvo2TctLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mantener 'NORDEMP' y 'CIIU 4' fuera\n",
        "nuid_column = Mod4['NORDEMP']\n",
        "ciiu_column = Mod4['CIIU 4']\n",
        "\n",
        "# Eliminar 'NORDEMP' y 'CIIU 4' del DataFrame para la reducción\n",
        "Mod4_red = Mod4.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Preprocesamiento: eliminar filas con valores faltantes\n",
        "Mod4_red.dropna(inplace=True)\n",
        "\n",
        "# Aplicar t-SNE para reducir la dimensionalidad a 2 componentes\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(Mod4_red)\n",
        "\n",
        "# Crear un DataFrame con los resultados de t-SNE\n",
        "df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])\n",
        "df_tsne['NORDEMP'] = nuid_column\n",
        "df_tsne['CIIU 4'] = ciiu_column\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', data=df_tsne, palette='viridis')\n",
        "plt.title('t-SNE results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x5Prcqwkcxbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "dkeFVQT2dMhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar DBSCAN para detectar outliers\n",
        "dbscan = DBSCAN(eps=3, min_samples=10)  # Ajusta los parámetros según tus datos\n",
        "df_tsne['outlier'] = dbscan.fit_predict(tsne_results)\n",
        "df_tsne['index'] = Mod4_red.index\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='TSNE1', y='TSNE2', hue='outlier', palette='viridis', data=df_tsne, legend='full')\n",
        "plt.title('t-SNE results with outliers marked')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hkeyDkA6dLlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminación de outliers"
      ],
      "metadata": {
        "id": "jQqtme6XdcLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los datos atípicos\n",
        "outliers = df_tsne[df_tsne['outlier'] == -1]\n",
        "print(\"Datos atípicos detectados:\")\n",
        "print(outliers)"
      ],
      "metadata": {
        "id": "QDh11iPLdf9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar los índices de los datos que no son outliers\n",
        "non_outliers_indices = df_tsne[df_tsne['outlier'] != -1]['index']\n",
        "\n",
        "# Crear un nuevo DataFrame sin los outliers\n",
        "Mod4_cleaned = Mod4.loc[non_outliers_indices]\n",
        "\n",
        "# Guardar el DataFrame limpio en un nuevo archivo Excel\n",
        "Mod4_cleaned.to_excel('Mod4_cleaned.xlsx', index=False)"
      ],
      "metadata": {
        "id": "e8icrzI1dlnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de Predicción"
      ],
      "metadata": {
        "id": "loMVwbM0BXSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar las hojas del archivo Excel\n",
        "file_path = 'Mod4_cleaned.xlsx'  # Asegúrate de subir el archivo a Colab o especificar la ruta correcta\n",
        "Mod4 = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "oj1sMetlBax9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las columnas 'NORDEMP' y 'CIIU 4'\n",
        "Mod4 = Mod4.drop(columns=['NORDEMP', 'CIIU 4'])\n",
        "\n",
        "# Seleccionar las características (todas menos 'I3R2C1') y la variable objetivo ('I3R2C1')\n",
        "X = Mod4.drop(columns=['I3R2C1'])\n",
        "y = Mod4['I3R2C1']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Dv1tYdSeCQXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo de Regresión Lineal\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_lin_reg = lin_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_lin_reg = mean_squared_error(y_test, y_pred_lin_reg)\n",
        "r2_lin_reg = r2_score(y_test, y_pred_lin_reg)\n",
        "\n",
        "print(\"Regresión Lineal\")\n",
        "print(\"Mean Squared Error:\", mse_lin_reg)\n",
        "print(\"R2 Score:\", r2_lin_reg)"
      ],
      "metadata": {
        "id": "8EP0q_hyPAXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de Random Forest\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"Mejores hiperparámetros:\", grid_search_rf.best_params_)\n",
        "print(\"Mean Squared Error:\", mse_rf)\n",
        "print(\"R2 Score:\", r2_rf)"
      ],
      "metadata": {
        "id": "DW-f9xAzPB_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de Random Forest con los mejores hiperparámetros\n",
        "best_rf = RandomForestRegressor(\n",
        "    max_depth=30,\n",
        "    max_features='auto',\n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=2,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "best_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_rf = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print(\"Mean Squared Error:\", mse_rf)\n",
        "print(\"R2 Score:\", r2_rf)"
      ],
      "metadata": {
        "id": "oTXY6paQpRjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de XGBoost\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Definir el grid de hiperparámetros\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "\n",
        "# Entrenar y evaluar el modelo con los mejores hiperparámetros\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"Mejores hiperparámetros:\", grid_search_xgb.best_params_)\n",
        "print(\"Mean Squared Error:\", mse_xgb)\n",
        "print(\"R2 Score:\", r2_xgb)\n"
      ],
      "metadata": {
        "id": "rL5HYcRsPVm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo de XGBoost con los mejores hiperparámetros\n",
        "best_xgb = xgb.XGBRegressor(\n",
        "    colsample_bytree=0.8,\n",
        "    learning_rate=0.2,\n",
        "    max_depth=7,\n",
        "    n_estimators=200,\n",
        "    subsample=0.7,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "best_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost\")\n",
        "print(\"Mean Squared Error:\", mse_xgb)\n",
        "print(\"R2 Score:\", r2_xgb)"
      ],
      "metadata": {
        "id": "HMrsMUYkpTML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función de creación de modelo\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        model.add(layers.Dense(\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        ))\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            model.add(layers.Dropout(rate=0.5))\n",
        "    model.add(layers.Dense(1))  # Salida para regresión\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        metrics=['mean_squared_error']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Búsqueda de hiperparámetros\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_mean_squared_error\",\n",
        "    max_trials=20,\n",
        "    directory=\"/content/tuning_neural_network2\",\n",
        "    project_name=\"NN_tuning\"\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Entrenar el modelo con los mejores hiperparámetros\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Evaluar el modelo\n",
        "eval_result = model.evaluate(X_test_scaled, y_test)\n",
        "\n",
        "# Precisión en los datos de prueba\n",
        "mse = eval_result[0]\n",
        "r2 = r2_score(y_test, model.predict(X_test_scaled))\n",
        "\n",
        "print('Mean Squared Error en los datos de prueba:', mse)\n",
        "print('R2 Score en los datos de prueba:', r2)"
      ],
      "metadata": {
        "id": "580zQ7_sPtvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}