{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonillahermes/Data_Science_Projects/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hermes Yate Bonilla\n",
        "**Data Scientist**\n",
        "---\n",
        "\n",
        "**Contact:**\n",
        "- **Email:** [bonillahermes@gmail.com](mailto:bonillahermes@gmail.com)\n",
        "- **LinkedIn:** [linkedin.com/in/bonillahermes](https://www.linkedin.com/in/bonillahermes/)\n",
        "- **GitHub:** [github.com/bonillahermes](https://github.com/bonillahermes)\n",
        "- **Webpage:** [bonillahermes.com](https://bonillahermes.com/)\n",
        "---"
      ],
      "metadata": {
        "id": "3GVDoYMj7t7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza de datos"
      ],
      "metadata": {
        "id": "dzIBdULPK6wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descripción de los Problemas y Soluciones en el Dataset\n",
        "\n",
        "1. **Valores Nulos en 'City' y 'State'**\n",
        "   - **Descripción del problema**: La columna `City` tenía valores nulos y la columna `State` también tenía algunos valores nulos.\n",
        "   - **Cómo se ha detectado**: Mediante el uso de `data.info()`, que muestra el recuento de valores no nulos en cada columna.\n",
        "   - **Por qué es un problema**: Los valores nulos pueden causar errores en el análisis y en ciertas operaciones de datos.\n",
        "   - **Ejemplo del dataset**: Cualquier registro donde `City` o `State` sean nulos.\n",
        "   - **Solución adoptada**: Imputación de valores nulos con el valor 'Desconocido'.\n",
        "   - **Justificación**: La imputación mantiene la integridad del dataset y permite realizar análisis sin interrupciones por falta de datos.\n",
        "\n",
        "2. **Columna 'Range' Vacía**\n",
        "   - **Descripción del problema**: La columna `Range` estaba completamente vacía sin valores no nulos.\n",
        "   - **Cómo se ha detectado**: Observando el recuento de valores no nulos mediante `data.info()`.\n",
        "   - **Por qué es un problema**: Una columna completamente vacía no aporta información y ocupa espacio innecesario.\n",
        "   - **Ejemplo del dataset**: La columna `Range` entera.\n",
        "   - **Solución adoptada**: Eliminar la columna `Range`.\n",
        "   - **Justificación**: Eliminar una columna completamente vacía reduce la complejidad del dataset y mejora la eficiencia del almacenamiento y procesamiento.\n",
        "\n",
        "3. **Inconsistencia en 'OriginalCrimeTypeName'**\n",
        "   - **Descripción del problema**: Presencia de valores numéricos, alfanuméricos y palabras incompletas en `OriginalCrimeTypeName`.\n",
        "   - **Cómo se ha detectado**: Al revisar los valores únicos y ejemplos específicos de esta columna.\n",
        "   - **Por qué es un problema**: Esto podría indicar errores de entrada de datos o falta de estandarización en la categorización de los crímenes.\n",
        "   - **Ejemplo del dataset**: Valores como \"500E\", \"917/811\", o \"JO\".\n",
        "   - **Solución adoptada**: Reemplazo de valores numéricos, alfanuméricos y palabras cortas con 'Desconocido' y normalización a mayúsculas.\n",
        "   - **Justificación**: Esto asegura consistencia y claridad en la categorización de los tipos de crimen, mejorando la calidad y uniformidad de los datos para análisis futuros.\n",
        "\n",
        "4. **Registros Duplicados**\n",
        "   - **Descripción del problema**: Posible presencia de registros duplicados en el dataset.\n",
        "   - **Cómo se ha detectado**: Al realizar una comprobación de duplicados en el dataset.\n",
        "   - **Por qué es un problema**: Los duplicados pueden distorsionar el análisis y llevar a conclusiones erróneas.\n",
        "   - **Ejemplo del dataset**: Cualquier par de filas idénticas en el dataset.\n",
        "   - **Solución adoptada**: Eliminación de registros duplicados.\n",
        "   - **Justificación**: Esto garantiza que cada registro en el dataset sea único, lo que es crucial para la precisión del análisis de datos.\n",
        "\n",
        "5. **Formato y Separación de Fecha y Hora en 'OffenseDate', 'CallTime' y 'CallDateTime'**\n",
        "   - **Descripción del problema**: Las columnas `OffenseDate`, `CallTime` y `CallDateTime` tienen formatos de fecha y hora mezclados que podrían no ser ideales para el análisis.\n",
        "   - **Cómo se ha detectado**: Al inspeccionar las columnas, se observó que las fechas y las horas estaban combinadas en una sola columna o en formatos que no facilitan el análisis.\n",
        "   - **Por qué es un problema**: Tener la fecha y la hora en la misma columna o en formatos no estándar dificulta la realización de análisis basados en tiempo y fecha.\n",
        "   - **Ejemplo del dataset**: Valores en `CallDateTime` como '2016-04-01T12:04:00', donde la fecha y la hora están combinadas.\n",
        "   - **Solución adoptada**:\n",
        "     - Convertir `OffenseDate` a formato de fecha y extraer la hora en una nueva columna `OffenseTime`.\n",
        "     - Convertir `CallDateTime` a formato de fecha y hora, luego separar en dos columnas: `CallDate` y `CallTime`.\n",
        "     - Eliminar la columna original `CallDateTime` después de la separación.\n",
        "   - **Justificación**: Separar las fechas y las horas en columnas distintas permite un análisis más fácil y flexible de los datos temporales. La estandarización del formato mejora la coherencia y la eficiencia en el manejo de los datos.\n",
        "\n",
        "6. **Eliminación de la Columna 'OffenseTime'**\n",
        "   - **Descripción del problema**: La columna `OffenseTime` solo contenía ceros, lo que indica que no proporcionaba información útil.\n",
        "   - **Cómo se ha detectado**: Al realizar la conversión y separación de la columna `OffenseDate`, se observó que la columna `OffenseTime` generada solo contenía valores cero.\n",
        "   - **Por qué es un problema**: Una columna que solo contiene un valor único (en este caso, cero) no aporta valor analítico y puede ser engañosa durante el análisis de datos.\n",
        "   - **Ejemplo del dataset**: La columna `OffenseTime` entera, donde todos los registros eran cero.\n",
        "   - **Solución adoptada**: Eliminar la columna `OffenseTime` del dataset.\n",
        "   - **Justificación**: Eliminar columnas que no aportan información útil reduce la complejidad del dataset, mejora la claridad y evita confusiones durante el análisis de datos. Además, ayuda a optimizar el uso de recursos de memoria y procesamiento.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ui720JJSOAu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "3X1-ldljaQth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt8rbOLSKxEj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Monta tu Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Uo2BErmK6GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar Base de Datos"
      ],
      "metadata": {
        "id": "HKY2Hc3XaWN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta del archivo Excel en un Google Drive\n",
        "ruta_archivo = '/content/drive/MyDrive/Bases/data_act_01.csv'\n",
        "\n",
        "# Cargar el archivo Excel en un DataFrame de Pandas\n",
        "data = pd.read_csv(ruta_archivo)\n",
        "data = pd.read_csv(ruta_archivo, sep=';')\n",
        "\n",
        "# Mostrar las primeras filas para entender la estructura\n",
        "data.head()"
      ],
      "metadata": {
        "id": "u5PLXdUwLAOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Información Adicional del Dataset"
      ],
      "metadata": {
        "id": "V8EJM4neaaok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Información general sobre el DataFrame\n",
        "info = data.info()"
      ],
      "metadata": {
        "id": "r44jHhnFLDok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de Valores Nulos"
      ],
      "metadata": {
        "id": "68zhp4i-afWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores nulos\n",
        "valores_nulos = data.isnull().sum()\n",
        "valores_nulos"
      ],
      "metadata": {
        "id": "_dpdq8SaLHKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputar valores nulos\n",
        "# Aquí estoy asumiendo que queremos llenar los valores nulos de 'City' y 'State' con una cadena 'Desconocido'\n",
        "data['City'].fillna('Desconocido', inplace=True)\n",
        "data['State'].fillna('Desconocido', inplace=True)"
      ],
      "metadata": {
        "id": "mPPnfihsMYOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizar Datos"
      ],
      "metadata": {
        "id": "VkPQ_wTiaoVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar datos\n",
        "# Convertir a mayúsculas algunas columnas para mantener la consistencia\n",
        "columns_to_normalize = ['OriginalCrimeTypeName', 'Disposition', 'Address', 'City', 'State', 'AddressType']\n",
        "data[columns_to_normalize] = data[columns_to_normalize].apply(lambda x: x.str.upper())"
      ],
      "metadata": {
        "id": "an1x_mpiMeVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpiar Columnas"
      ],
      "metadata": {
        "id": "o48gvwb9araO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para limpiar la columna 'OriginalCrimeTypeName'\n",
        "def clean_crime_type_name(value):\n",
        "    if isinstance(value, str):\n",
        "        if value.isdigit() or len(value) < 3 or '/' in value:\n",
        "            return 'Desconocido'\n",
        "        else:\n",
        "            return value.upper()\n",
        "    return value\n",
        "\n",
        "# Aplicar la función de limpieza a 'OriginalCrimeTypeName'\n",
        "data['OriginalCrimeTypeName'] = data['OriginalCrimeTypeName'].apply(clean_crime_type_name)"
      ],
      "metadata": {
        "id": "hvO0Duc8PZNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminar Variable Sin Asignaciones"
      ],
      "metadata": {
        "id": "krKDDAYVavMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar la columna 'Range'\n",
        "data.drop('Range', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XHk8mRYMNEjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eliminar Duplicados"
      ],
      "metadata": {
        "id": "t4f4KOBbazv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar el número de filas antes de eliminar duplicados\n",
        "num_rows_before = data.shape[0]\n",
        "print(num_rows_before)\n",
        "\n",
        "# Eliminar duplicados\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Contar el número de filas después de eliminar duplicados\n",
        "num_rows_after = data.shape[0]\n",
        "print(num_rows_after)\n",
        "\n",
        "# Calcular el número de filas duplicadas eliminadas\n",
        "num_duplicates_removed = num_rows_before - num_rows_after\n",
        "\n",
        "print(f\"Número de filas duplicadas eliminadas: {num_duplicates_removed}\")"
      ],
      "metadata": {
        "id": "KzDnWueURpMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convertir y Separar Variables"
      ],
      "metadata": {
        "id": "8L3H2-6Fa3fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir y separar las columnas de fecha y hora\n",
        "data['OffenseDate'] = pd.to_datetime(data['OffenseDate']).dt.date\n",
        "data['OffenseTime'] = pd.to_datetime(data['OffenseDate']).dt.time\n",
        "data['CallDateTime'] = pd.to_datetime(data['CallDateTime'])\n",
        "data['CallDate'] = data['CallDateTime'].dt.date\n",
        "data['CallTime'] = data['CallDateTime'].dt.time\n",
        "data.drop('CallDateTime', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "GssStuu6MjAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardar Base de Datos"
      ],
      "metadata": {
        "id": "LDAUGZBebHc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los datos limpios en un nuevo archivo CSV\n",
        "data.to_csv('data_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "y_FwyocaMnj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Guardar en Formato JSON**"
      ],
      "metadata": {
        "id": "3jT626T1btAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Cargar y limpiar los datos (asumiendo que las transformaciones ya se han realizado)\n",
        "data_cleaned = pd.read_csv('data_cleaned.csv')\n",
        "\n",
        "# Convertir cada fila del DataFrame a un documento JSON\n",
        "json_data = data_cleaned.apply(lambda x: x.to_json(), axis=1)\n",
        "\n",
        "# Guardar los datos JSON en un archivo (opcional)\n",
        "with open('data_cleaned.json', 'w') as file:\n",
        "    file.write(json_data.to_json(orient='records'))\n",
        "\n",
        "# Visualizar los primeros documentos JSON como ejemplo\n",
        "print(json_data.head())\n"
      ],
      "metadata": {
        "id": "nKqYnx3Pb0S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metodología de Limpieza de Datos\n",
        "\n",
        "Según lo observado en el dataset, se propone la siguiente metodología:\n",
        "\n",
        "\n",
        "1. **Evaluación Inicial del Dataset**:\n",
        "   - Realizar una revisión preliminar para entender la estructura, dimensiones y tipos de datos (numéricos, textuales, fechas, etc.).\n",
        "   - Identificar formatos de archivo y herramientas necesarias para manipular los datos (por ejemplo, hojas de cálculo, bases de datos, software de análisis de datos).\n",
        "\n",
        "2. **Corrección de Formatos y Delimitadores**:\n",
        "   - Verificar y ajustar, si es necesario, los delimitadores y formatos de archivo para garantizar que los datos estén correctamente estructurados y accesibles.\n",
        "\n",
        "3. **Tratamiento de Valores Faltantes o Nulos**:\n",
        "   - Identificar columnas con valores faltantes o nulos.\n",
        "   - Decidir sobre la imputación o eliminación de estos valores basándose en el contexto y relevancia para el análisis.\n",
        "\n",
        "4. **Normalización de Datos Textuales**:\n",
        "   - Estandarizar textos para mantener la consistencia (por ejemplo, unificar mayúsculas/minúsculas, corregir errores tipográficos).\n",
        "   - Unificar términos y categorías similares para evitar duplicidades.\n",
        "\n",
        "5. **Manejo de Datos Atípicos o Anómalos**:\n",
        "   - Identificar datos que no se ajustan a los patrones esperados o que son extremos.\n",
        "   - Evaluar su impacto y decidir sobre su ajuste o eliminación.\n",
        "\n",
        "6. **Validación de la Integridad y Lógica de los Datos**:\n",
        "   - Asegurar que los datos cumplan con las reglas de negocio o lógica inherente (por ejemplo, secuencia cronológica en fechas, coherencia en identificadores).\n",
        "\n",
        "7. **Eliminación de Duplicados**:\n",
        "   - Detectar y eliminar registros duplicados para asegurar la unicidad y precisión en los datos.\n",
        "\n",
        "8. **Transformación y Creación de Nuevas Variables**:\n",
        "   - Desarrollar nuevas variables o transformar las existentes para facilitar análisis específicos (por ejemplo, categorización, bandas de edad).\n",
        "\n",
        "9. **Documentación del Proceso de Limpieza**:\n",
        "   - Registrar los pasos realizados y las decisiones tomadas durante la limpieza para garantizar la reproducibilidad y comprensión del proceso.\n",
        "\n",
        "10. **Almacenamiento y Preservación de los Datos Limpio**:\n",
        "    - Guardar el dataset limpio en un formato adecuado para su uso futuro.\n",
        "    - Asegurar que los datos estén almacenados de manera segura y accesible.\n",
        "\n"
      ],
      "metadata": {
        "id": "5zUvIC1ZeF1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Propuestas de Mejoras para el Conjunto de Datos\n",
        "\n",
        "## Mejora 1: Estandarización y Validación en la Captura de Datos\n",
        "\n",
        "1. **Descripción de la Mejora**:\n",
        "   - Implementar un sistema integral de validación y estandarización directamente en el punto de captura de datos. Este sistema garantizará la coherencia y la exactitud de los datos desde el inicio, reduciendo errores y datos atípicos.\n",
        "\n",
        "2. **Procedimiento de Implementación**:\n",
        "   - **Desarrollo de Reglas de Validación**: Establecer un conjunto claro de normas para cada tipo de dato (p.ej., formatos estandarizados para fechas, números y texto).\n",
        "   - **Integración en el Sistema de Captura de Datos**: Incorporar estas reglas en las interfaces de entrada de datos, como formularios web o aplicaciones móviles.\n",
        "   - **Retroalimentación en Tiempo Real**: Configurar el sistema para que proporcione alertas inmediatas en caso de datos mal ingresados, permitiendo correcciones al momento.\n",
        "   - **Capacitación y Concienciación**: Realizar sesiones de formación con los usuarios que ingresan datos para enfatizar la importancia de la precisión y mostrar cómo usar el sistema de manera efectiva.\n",
        "\n",
        "## Mejora 2: Auditoría y Limpieza Periódica de Datos\n",
        "\n",
        "1. **Descripción de la Mejora**:\n",
        "   - Establecer un protocolo de auditoría y limpieza de datos regular para revisar y corregir problemas en los datos almacenados. Esto incluirá la identificación de duplicados, correcciones de inexactitudes y actualizaciones de datos obsoletos.\n",
        "\n",
        "2. **Procedimiento de Implementación**:\n",
        "   - **Planificación de Auditorías**: Definir un calendario fijo para la revisión de datos (p.ej., cada trimestre).\n",
        "   - **Herramientas de Análisis de Datos**: Utilizar software especializado para detectar y resolver problemas comunes en los datos.\n",
        "   - **Registro y Seguimiento**: Mantener un historial de todas las auditorías realizadas, incluyendo los problemas encontrados y las soluciones aplicadas.\n",
        "   - **Revisión Continua de Procesos**: Basándose en los resultados de las auditorías, ajustar y mejorar las reglas de validación y los procesos de captura de datos.\n",
        "\n",
        "---\n",
        "\n",
        "Estas mejoras, al ser implementadas y mantenidas de manera continua, asegurarán la alta calidad del conjunto de datos a lo largo del tiempo. Esto no solo reduce la carga de trabajo asociada con la limpieza de datos posterior, sino que también aumenta significativamente la confiabilidad y utilidad de los datos para el análisis y la toma de decisiones.\n"
      ],
      "metadata": {
        "id": "d43Eb99VgkZY"
      }
    }
  ]
}